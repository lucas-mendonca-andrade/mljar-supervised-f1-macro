#!/usr/bin/env python3
"""
Teste para verificar se a m√©trica f1_macro est√° funcionando corretamente.
"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from supervised import AutoML

def test_f1_macro_binary_classification():
    """Teste para classifica√ß√£o bin√°ria com f1_macro"""
    print("=== Teste de Classifica√ß√£o Bin√°ria com f1_macro ===")
    
    # Criar dados sint√©ticos para classifica√ß√£o bin√°ria
    X, y = make_classification(
        n_samples=1000,
        n_features=10,
        n_informative=5,
        n_redundant=2,
        n_classes=2,
        random_state=42,
        class_sep=0.8
    )
    
    # Converter para DataFrame
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
    y = pd.Series(y)
    
    # Dividir em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Forma dos dados: {X.shape}")
    print(f"Distribui√ß√£o das classes: {np.bincount(y)}")
    
    # Treinar AutoML com f1_macro
    automl = AutoML(
        mode="Explain",
        eval_metric="f1_macro",
        results_path="test_f1_macro_binary_new",
        total_time_limit=60,  # Limitar tempo para teste r√°pido
        train_ensemble=False,
        stack_models=False
    )
    
    print("Treinando AutoML com f1_macro...")
    automl.fit(X_train, y_train)
    
    # Fazer predi√ß√µes
    predictions = automl.predict(X_test)
    
    # Calcular f1_macro manualmente para compara√ß√£o
    f1_macro_manual = f1_score(y_test, predictions, average='macro')
    
    print(f"F1 Macro calculado manualmente: {f1_macro_manual:.4f}")
    print(f"Melhor modelo: {automl.get_leaderboard().iloc[0]['name']}")
    
    # Corrigir a formata√ß√£o do score
    leaderboard = automl.get_leaderboard()
    best_score = leaderboard.iloc[0]['metric_value']
    if isinstance(best_score, str):
        try:
            best_score = float(best_score)
        except ValueError:
            best_score = 0.0
    print(f"Score do melhor modelo: {best_score:.4f}")
    
    return f1_macro_manual > 0.5  # Esperamos um score razo√°vel

def test_f1_macro_multiclass_classification():
    """Teste para classifica√ß√£o multiclasse com f1_macro"""
    print("\n=== Teste de Classifica√ß√£o Multiclasse com f1_macro ===")
    
    # Criar dados sint√©ticos para classifica√ß√£o multiclasse
    X, y = make_classification(
        n_samples=1500,
        n_features=10,
        n_informative=5,
        n_redundant=2,
        n_classes=3,
        random_state=42,
        class_sep=0.8
    )
    
    # Converter para DataFrame
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
    y = pd.Series(y)
    
    # Dividir em treino e teste
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Forma dos dados: {X.shape}")
    print(f"Distribui√ß√£o das classes: {np.bincount(y)}")
    
    # Treinar AutoML com f1_macro
    automl = AutoML(
        mode="Explain",
        eval_metric="f1_macro",
        results_path="test_f1_macro_multiclass_new",
        total_time_limit=60,  # Limitar tempo para teste r√°pido
        train_ensemble=False,
        stack_models=False
    )
    
    print("Treinando AutoML com f1_macro...")
    automl.fit(X_train, y_train)
    
    # Fazer predi√ß√µes
    predictions = automl.predict(X_test)
    
    # Calcular f1_macro manualmente para compara√ß√£o
    f1_macro_manual = f1_score(y_test, predictions, average='macro')
    
    print(f"F1 Macro calculado manualmente: {f1_macro_manual:.4f}")
    print(f"Melhor modelo: {automl.get_leaderboard().iloc[0]['name']}")
    
    # Corrigir a formata√ß√£o do score
    leaderboard = automl.get_leaderboard()
    best_score = leaderboard.iloc[0]['metric_value']
    if isinstance(best_score, str):
        try:
            best_score = float(best_score)
        except ValueError:
            best_score = 0.0
    print(f"Score do melhor modelo: {best_score:.4f}")
    
    return f1_macro_manual > 0.4  # Esperamos um score razo√°vel

def test_metric_validation():
    """Teste para verificar se a valida√ß√£o de m√©tricas est√° funcionando"""
    print("\n=== Teste de Valida√ß√£o de M√©tricas ===")
    
    try:
        # Tentar usar f1_macro em classifica√ß√£o bin√°ria (deve funcionar)
        automl = AutoML(eval_metric="f1_macro")
        print("‚úì f1_macro √© aceito para classifica√ß√£o bin√°ria")
        
        # Tentar usar f1_macro em classifica√ß√£o multiclasse (deve funcionar)
        automl = AutoML(eval_metric="f1_macro")
        print("‚úì f1_macro √© aceito para classifica√ß√£o multiclasse")
        
        # Tentar usar uma m√©trica inv√°lida (deve falhar)
        try:
            # Criar dados para for√ßar a valida√ß√£o
            X, y = make_classification(n_samples=100, n_features=5, n_classes=2, random_state=42)
            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
            y = pd.Series(y)
            
            automl = AutoML(eval_metric="invalid_metric")
            automl.fit(X, y)  # Isso deve falhar na valida√ß√£o
            print("‚úó M√©trica inv√°lida foi aceita (erro esperado)")
            return False
        except ValueError as e:
            if "invalid_metric" in str(e):
                print("‚úì M√©trica inv√°lida foi rejeitada corretamente")
            else:
                print(f"‚úó Erro inesperado: {e}")
                return False
        
        return True
        
    except Exception as e:
        print(f"‚úó Erro na valida√ß√£o de m√©tricas: {e}")
        return False

if __name__ == "__main__":
    print("Iniciando testes para f1_macro...")
    
    # Executar testes
    test1_passed = test_f1_macro_binary_classification()
    test2_passed = test_f1_macro_multiclass_classification()
    test3_passed = test_metric_validation()
    
    print("\n" + "="*50)
    print("RESULTADOS DOS TESTES:")
    print(f"Teste 1 (Classifica√ß√£o Bin√°ria): {'‚úì PASSOU' if test1_passed else '‚úó FALHOU'}")
    print(f"Teste 2 (Classifica√ß√£o Multiclasse): {'‚úì PASSOU' if test2_passed else '‚úó FALHOU'}")
    print(f"Teste 3 (Valida√ß√£o de M√©tricas): {'‚úì PASSOU' if test3_passed else '‚úó FALHOU'}")
    
    all_passed = test1_passed and test2_passed and test3_passed
    print(f"\nStatus Geral: {'‚úì TODOS OS TESTES PASSARAM' if all_passed else '‚úó ALGUNS TESTES FALHARAM'}")
    
    if all_passed:
        print("\nüéâ A m√©trica f1_macro est√° funcionando corretamente!")
    else:
        print("\n‚ö†Ô∏è  H√° problemas com a implementa√ß√£o da m√©trica f1_macro.") 